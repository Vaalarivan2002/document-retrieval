import os
import sys
import logging
import torch
import errno
from typing import Union, Tuple, List, Dict
from collections import defaultdict
import transformers
from transformers import BertModel, XLMRobertaModel, AutoTokenizer

def load_hf(object_class, model_name):
    try:
        obj = object_class.from_pretrained(model_name, local_files_only=True)
    except:
        obj = object_class.from_pretrained(model_name, local_files_only=False)
    return obj

class Contriever(BertModel):
    def __init__(self, config, pooling="average", **kwargs):
        super().__init__(config, add_pooling_layer=False)
        if not hasattr(config, "pooling"):
            self.config.pooling = pooling

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        output_attentions=None,
        output_hidden_states=None,
        normalize=False,
    ):

        model_output = super().forward(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )

        last_hidden = model_output["last_hidden_state"]
        last_hidden = last_hidden.masked_fill(~attention_mask[..., None].bool(), 0.0)

        if self.config.pooling == "average":
            emb = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]
        elif self.config.pooling == "cls":
            emb = last_hidden[:, 0]

        if normalize:
            emb = torch.nn.functional.normalize(emb, dim=-1)
        return emb

class XLMRetriever(XLMRobertaModel):
    def __init__(self, config, pooling="average", **kwargs):
        super().__init__(config, add_pooling_layer=False)
        if not hasattr(config, "pooling"):
            self.config.pooling = pooling

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        output_attentions=None,
        output_hidden_states=None,
        normalize=False,
    ):

        model_output = super().forward(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_attention_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )

        last_hidden = model_output["last_hidden_state"]
        last_hidden = last_hidden.masked_fill(~attention_mask[..., None].bool(), 0.0)
        if self.config.pooling == "average":
            emb = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]
        elif self.config.pooling == "cls":
            emb = last_hidden[:, 0]
        if normalize:
            emb = torch.nn.functional.normalize(emb, dim=-1)
        return emb

def load_retriever(model_path, pooling="average", random_init=False):
    path = os.path.join(model_path, "checkpoint.pth")
    if os.path.exists(path):
        pretrained_dict = torch.load(path, map_location="cpu")
        opt = pretrained_dict["opt"]
        if hasattr(opt, "retriever_model_id"):
            retriever_model_id = opt.retriever_model_id
        else:
            retriever_model_id = "bert-base-multilingual-cased"
        tokenizer = load_hf(transformers.AutoTokenizer, retriever_model_id)
        cfg = load_hf(transformers.AutoConfig, retriever_model_id)
        if "xlm" in retriever_model_id:
            model_class = XLMRetriever
        else:
            model_class = Contriever
        retriever = model_class(cfg)
        pretrained_dict = pretrained_dict["model"]

        if any("encoder_q." in key for key in pretrained_dict.keys()):  # test if model is defined with moco class
            pretrained_dict = {k.replace("encoder_q.", ""): v for k, v in pretrained_dict.items() if "encoder_q." in k}
        elif any("encoder." in key for key in pretrained_dict.keys()):  # test if model is defined with inbatch class
            pretrained_dict = {k.replace("encoder.", ""): v for k, v in pretrained_dict.items() if "encoder." in k}
        retriever.load_state_dict(pretrained_dict, strict=False)
    else:
        retriever_model_id = model_path
        if "xlm" in retriever_model_id:
            model_class = XLMRetriever
        else:
            model_class = Contriever
        cfg = load_hf(transformers.AutoConfig, model_path)
        tokenizer = load_hf(transformers.AutoTokenizer, model_path)
        retriever = load_hf(model_class, model_path)

    return retriever, tokenizer, retriever_model_id

contriever = Contriever.from_pretrained("facebook/contriever")
tokenizer = AutoTokenizer.from_pretrained("facebook/contriever")

sentences = [
    "When was Marie Curie born?",
    "Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.",
    "Born in Paris on 15 May 1859, Pierre Curie was the son of Eug√®ne Curie, a doctor of French Catholic origin from Alsace."
]

inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors="pt")
embeddings = contriever(**inputs)
print(embeddings)
score01 = embeddings[0] @ embeddings[1]
score02 = embeddings[0] @ embeddings[2]

print(score01)
print(score02)


def logistic_loss(scores, labels):
    """
    Calculates the logistic loss.

    Args:
        scores: A tensor of scores.
        labels: A tensor of labels (0 or 1).

    Returns:
        The logistic loss.
    """
    loss_fn = torch.nn.BCEWithLogitsLoss()

    return loss_fn(scores, labels.float())


score01 = torch.tensor(1.0473)
score02 = torch.tensor(1.0095)



label01 = torch.tensor(1)
label02 = torch.tensor(0)

loss1 = logistic_loss(score01, label01)
loss2 = logistic_loss(score02, label02)


print(f"Logistic loss for score01: {loss1}")
print(f"Logistic loss for score02: {loss2}")


total_loss = (loss1 + loss2) / 2
print(f"Average Logistic Loss: {total_loss}")


def triplet_loss(anchor, positive, negative, margin=0.5):
    """
    Calculates the triplet loss.

    Args:
        anchor: Embedding of the anchor sample.
        positive: Embedding of the positive sample.
        negative: Embedding of the negative sample.
        margin: Margin for the triplet loss.

    Returns:
        The triplet loss.
    """
    distance_positive = torch.norm(anchor - positive, p=2)
    distance_negative = torch.norm(anchor - negative, p=2)
    loss = torch.max(torch.tensor(0.0), distance_positive - distance_negative + margin)
    return loss


anchor_embedding = embeddings[0]
positive_embedding = embeddings[1]
negative_embedding = embeddings[2]


loss = triplet_loss(anchor_embedding, positive_embedding, negative_embedding)
print(f"Triplet Loss: {loss}")
